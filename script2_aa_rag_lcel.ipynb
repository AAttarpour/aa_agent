{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d86963ea",
   "metadata": {},
   "source": [
    "- This code has written by Ahmadreza Attarpour (a.attarpour@mail.utoronto.ca)\n",
    "- It's a practice to check how we can use LangChain/LangGraph to build RAG\n",
    "\n",
    "-------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb35a012",
   "metadata": {},
   "source": [
    "# RAG\n",
    "\n",
    "- Retrieval-Augmented Generation (RAG) is a method that improves AI responses by combining a language model with a search system. Instead of relying only on what the model \"knows,\" RAG looks up relevant information (from documents, websites, or databases) and uses it to give more accurate and current answers. In LangChain, LCEL (LangChain Expression Language) helps connect these steps in a flexible and modular way.\n",
    "\n",
    "- LCEL is designed to streamline the process of building useful apps with LLMs and combining related components.\n",
    "\n",
    "- Langgraph, built on top of LCEL, allows for performant orchestrations fo application components while maintaining concise and readable code. \n",
    "\n",
    "User Question\n",
    "\n",
    "     ↓\n",
    "\n",
    "[ Retriever → Fetch Relevant Info ]\n",
    "\n",
    "     ↓\n",
    "\n",
    "[ LLM → Use Retrieved Info to Generate Answer ]\n",
    "\n",
    "     ↓\n",
    "\n",
    "AI Response\n",
    "\n",
    "\n",
    "- RAG operates in one single direction; there is no loop or thinking and that's its disadvantage.\n",
    "\n",
    "# Simplified I/O Flow:\n",
    "\n",
    "**Input:** \"What is llama3?\"\n",
    "\n",
    "⬇️\n",
    "\n",
    "1. Convert question to embedding\n",
    "2. Retrieve top-4 similar chunks\n",
    "3. Plug into prompt:\n",
    "   - context = [relevant chunk 1...4]\n",
    "   - question = original question\n",
    "4. Feed to LLM\n",
    "\n",
    "⬇️\n",
    "\n",
    "**Output:** Final answer generated using retrieved context\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a19adb1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6ba50a41",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load the environment variables from the .env file\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "03cd57a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the variable from the environment and set it as a env variable\n",
    "GOOGLE_API_KEY = os.getenv(\"GOOGLE_API_KEY\")\n",
    "LANGCHAIN_API_KEY = os.getenv(\"LANGCHAIN_API_KEY\")\n",
    "LANGCHAIN_PROJECT = os.getenv(\"LANGCHAIN_PROJECT\")\n",
    "if GOOGLE_API_KEY:\n",
    "    os.environ[\"GOOGLE_API_KEY\"] = GOOGLE_API_KEY\n",
    "if LANGCHAIN_API_KEY:\n",
    "    os.environ[\"LANGCHAIN_API_KEY\"] = LANGCHAIN_API_KEY\n",
    "if LANGCHAIN_PROJECT:\n",
    "    os.environ[\"LANGCHAIN_PROJECT\"] = LANGCHAIN_PROJECT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2b55a8e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import TextLoader, DirectoryLoader \n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain import PromptTemplate\n",
    "from langchain_core.runnables import RunnableParallel, RunnablePassthrough, RunnableLambda\n",
    "from langchain_core.output_parsers import StrOutputParser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4c91e0b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the LLM and embeddings from langchain_google_genai\n",
    "from langchain_google_genai import GoogleGenerativeAIEmbeddings\n",
    "embeddings = GoogleGenerativeAIEmbeddings(model=\"models/embedding-001\")\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "llm = ChatGoogleGenerativeAI(model=\"gemini-2.0-flash\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "956e6a28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of documents loaded: 1\n",
      "First document content: Llama (Large Language Model Meta AI) is a family of autoregressive large language models released by...\n",
      "Number of documents after splitting: 39\n",
      "First document chunk: Llama (Large Language Model Meta AI) is a family...\n",
      "[Document(metadata={'source': 'data/llama3.txt'}, page_content='Llama (Large Language Model Meta AI) is a family'), Document(metadata={'source': 'data/llama3.txt'}, page_content='a family of autoregressive large language models'), Document(metadata={'source': 'data/llama3.txt'}, page_content='models released by Meta AI starting in February'), Document(metadata={'source': 'data/llama3.txt'}, page_content='February 2023.[2][3] The latest version is Llama'), Document(metadata={'source': 'data/llama3.txt'}, page_content='is Llama 3 released in April 2024.[4]'), Document(metadata={'source': 'data/llama3.txt'}, page_content='Model weights for the first version of Llama were'), Document(metadata={'source': 'data/llama3.txt'}, page_content='were released to the research community under a'), Document(metadata={'source': 'data/llama3.txt'}, page_content='under a non-commercial license.[5][3] Subsequent'), Document(metadata={'source': 'data/llama3.txt'}, page_content='versions of Llama were made accessible outside'), Document(metadata={'source': 'data/llama3.txt'}, page_content='outside academia and released under licenses that'), Document(metadata={'source': 'data/llama3.txt'}, page_content='that permitted some commercial use.[6][7] Llama'), Document(metadata={'source': 'data/llama3.txt'}, page_content='Llama models are trained at different parameter'), Document(metadata={'source': 'data/llama3.txt'}, page_content='parameter sizes, typically ranging between 7B and'), Document(metadata={'source': 'data/llama3.txt'}, page_content='7B and 70B.[4] Originally, Llama was only'), Document(metadata={'source': 'data/llama3.txt'}, page_content='was only available as a foundation model.[8]'), Document(metadata={'source': 'data/llama3.txt'}, page_content='model.[8] Starting with Llama 2, Meta AI started'), Document(metadata={'source': 'data/llama3.txt'}, page_content='started releasing instruction fine-tuned versions'), Document(metadata={'source': 'data/llama3.txt'}, page_content='versions alongside foundation models.[7]'), Document(metadata={'source': 'data/llama3.txt'}, page_content='Llama models have been compared favorably against'), Document(metadata={'source': 'data/llama3.txt'}, page_content='against other large language models. Meta AI'), Document(metadata={'source': 'data/llama3.txt'}, page_content='Meta AI reported the original 13B parameter'), Document(metadata={'source': 'data/llama3.txt'}, page_content=\"parameter model's performance on most NLP\"), Document(metadata={'source': 'data/llama3.txt'}, page_content='most NLP benchmarks exceeded that of the much'), Document(metadata={'source': 'data/llama3.txt'}, page_content='the much larger GPT-3 (with 175B parameters) and'), Document(metadata={'source': 'data/llama3.txt'}, page_content='and that the largest model was competitive with'), Document(metadata={'source': 'data/llama3.txt'}, page_content='with state of the art models such as PaLM and'), Document(metadata={'source': 'data/llama3.txt'}, page_content=\"PaLM and Chinchilla.[2]. Meta AI's testing shows\"), Document(metadata={'source': 'data/llama3.txt'}, page_content='shows that Llama 3 70B beats Gemini, and Claude'), Document(metadata={'source': 'data/llama3.txt'}, page_content='Claude in most benchmarks.[9][10] Wired describes'), Document(metadata={'source': 'data/llama3.txt'}, page_content='describes the 8B parameter version of Llama 3 as'), Document(metadata={'source': 'data/llama3.txt'}, page_content='3 as being \"surprisingly capable\" given it\\'s'), Document(metadata={'source': 'data/llama3.txt'}, page_content=\"it's size.[11]\"), Document(metadata={'source': 'data/llama3.txt'}, page_content='Alongside the release of Llama 3, Meta added'), Document(metadata={'source': 'data/llama3.txt'}, page_content='added virtual assistant features to Facebook and'), Document(metadata={'source': 'data/llama3.txt'}, page_content='and WhatsApp in select regions, and a standalone'), Document(metadata={'source': 'data/llama3.txt'}, page_content='website. Both services use a Llama 3 model.[12]'), Document(metadata={'source': 'data/llama3.txt'}, page_content='Reception was mixed, with some users confused'), Document(metadata={'source': 'data/llama3.txt'}, page_content='confused after Meta AI told a parental group that'), Document(metadata={'source': 'data/llama3.txt'}, page_content='that it had a child.[13]')]\n"
     ]
    }
   ],
   "source": [
    "# reading the txt files from the source directory\n",
    "loader = DirectoryLoader('./data', glob=\"./llama*.txt\", loader_cls=TextLoader)\n",
    "docs = loader.load()\n",
    "print(f\"Number of documents loaded: {len(docs)}\")\n",
    "print(f\"First document content: {docs[0].page_content[:100]}...\")  # Print first 100 characters of the first document\n",
    "# splitting the documents into smaller chunks\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=50,\n",
    "    chunk_overlap=10,\n",
    "    length_function=len\n",
    ")\n",
    "new_docs = text_splitter.split_documents(docs)\n",
    "doc_strings = [doc.page_content for doc in new_docs]\n",
    "print(f\"Number of documents after splitting: {len(doc_strings)}\")\n",
    "print(f\"First document chunk: {doc_strings[0][:100]}...\")  # Print first 100 characters of the first chunk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75abd560",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of documents in the vector store: 39\n"
     ]
    }
   ],
   "source": [
    "# creating Retiever using Embeddings and Chroma\n",
    "# Each chunk is turned into a vector (embedding) using the embeddings model.\n",
    "# The Chroma vector store saves these embeddings for future retrieval.\n",
    "# Creates a retriever to fetch top-4 (k=4) most relevant chunks based on similarity to a query.\n",
    "db = Chroma.from_documents(new_docs, embeddings)\n",
    "retriever = db.as_retriever(search_kwargs={\"k\": 4})\n",
    "print(f\"Number of documents in the vector store: {len(db)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "3f754c99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt template created: Use the following pieces of context to answer the question at the end.\n",
      "{context}\n",
      "\n",
      "Question: {questio...\n"
     ]
    }
   ],
   "source": [
    "# creating prompt template\n",
    "template = \"\"\"Use the following pieces of context to answer the question at the end.\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\"\"\"\n",
    "prompt = PromptTemplate.from_template(template)\n",
    "print(f\"Prompt template created: {prompt.template[:100]}...\")  # Print first 100 characters of the prompt template\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "f2085d7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LCEL\n",
    "\"\"\"Question → [Retriever gets context]  \n",
    "        → [PromptTemplate fills {context}, {question}]  \n",
    "        → [LLM answers based on it]  \n",
    "        → [OutputParser returns the response]\"\"\"\n",
    "\n",
    "retrieval_chain = (\n",
    "\n",
    "    RunnableParallel({\"context\": retriever, \"question\": RunnablePassthrough()})\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "32241716",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: when did llama release for the first time?\n",
      "Based on the provided text, the first release of Llama was in February 2023.\n"
     ]
    }
   ],
   "source": [
    "question = \"when did llama release for the first time?\"\n",
    "print(f\"Question: {question}\")\n",
    "print(retrieval_chain.invoke(question))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bedcae5",
   "metadata": {},
   "source": [
    "# Below are some hands on utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5208bc2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HELLO WORLD\n"
     ]
    }
   ],
   "source": [
    "# with Runnable components we can create a chain\n",
    "def string_to_uppercase(input_string: str) -> str:\n",
    "    return input_string.upper()\n",
    "\n",
    "chain = (\n",
    "    RunnablePassthrough() # does nothing, just passes the input through\n",
    "    | RunnableLambda(string_to_uppercase) # applies the string_to_uppercase function\n",
    ")\n",
    "\n",
    "print(chain.invoke(\"hello world\"))  # Should print \"HELLO WORLD\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24e2f56f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'uppercase': 'HELLO WORLD', 'lowercase': 'hello world'}\n"
     ]
    }
   ],
   "source": [
    "# we can create a chain with runnableparallel\n",
    "\n",
    "chain = RunnableParallel({\n",
    "    \"uppercase\": RunnableLambda(string_to_uppercase),\n",
    "    \"lowercase\": RunnableLambda(lambda x: x.lower())\n",
    "})\n",
    "print(chain.invoke(\"Hello World\"))  # Should print {'uppercase': 'HELLO WORLD', 'lowercase': 'hello world'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "4a090761",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'x': {'email_address': 'user@example.com', 'name': 'John Doe'}, 'email address': 'user@example.com'}\n"
     ]
    }
   ],
   "source": [
    "# let's write an example that gets a dictionary from the user and summarize it\n",
    "\n",
    "chain = RunnableParallel({\n",
    "    \"x\": RunnablePassthrough(),  # Just passes the input through),\n",
    "    \"email address\": lambda x: x['email_address']\n",
    "})\n",
    "\n",
    "print(chain.invoke({\"email_address\": \"user@example.com\", \"name\": \"John Doe\"}))  # Should print {'x': {'email_address': '\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e90039dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can donwload the model from Hugging Face and use it with langchain\n",
    "\"\"\"\n",
    "from langchain.embeddings import HuggingFaceBgeEmbeddings\n",
    "model_name = \"BAAI/bge-base-en-v1.5\"\n",
    "model_kwargs = ['device': 'cpu'}\n",
    "encode_kwargs = {'normalize_embeddings': True} # set True to compute cosine similarity\n",
    "embeddings = HuggingFaceBgeEmbeddings(\n",
    "model_name=model_name, model_kwargs=model_kwargs,\n",
    "encode_kwargs=encode_kwargs)\n",
    "\n",
    "\"\"\"\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aa_agent",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
